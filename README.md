# Procesamiento NLP - Documentos de BiologÃ­a Espacial NASA

## ğŸ“ DescripciÃ³n

Este proyecto realiza un anÃ¡lisis exhaustivo de Procesamiento de Lenguaje Natural (NLP) sobre documentos cientÃ­ficos de la NASA relacionados con biologÃ­a espacial. El objetivo es identificar tÃ³picos relevantes, frecuencias de tÃ©rminos y patrones en la investigaciÃ³n espacial.

## ğŸ¯ Objetivo

CompeticiÃ³n NASA - AnÃ¡lisis de documentos cientÃ­ficos para identificar tendencias y tÃ³picos principales en investigaciÃ³n de biologÃ­a espacial.

## ğŸ”„ Pipeline de Procesamiento

1. **Carga de Datos**: ImportaciÃ³n de archivos JSON con estudios de la NASA
2. **ExploraciÃ³n**: AnÃ¡lisis de la estructura y campos disponibles
3. **Limpieza**: 
   - EliminaciÃ³n de URLs, menciones, nÃºmeros
   - NormalizaciÃ³n de texto
   - EliminaciÃ³n de puntuaciÃ³n
4. **TokenizaciÃ³n**: SeparaciÃ³n del texto en palabras individuales
5. **Filtrado**: EliminaciÃ³n de stopwords (espaÃ±ol e inglÃ©s)
6. **LematizaciÃ³n**: ReducciÃ³n de palabras a su forma base usando spaCy
7. **AnÃ¡lisis de Frecuencias**: Conteo y ranking de tÃ©rminos
8. **VisualizaciÃ³n**:
   - GrÃ¡ficos de barras (top 30 palabras)
   - Nubes de palabras (WordCloud)
   - AnÃ¡lisis de bigramas
9. **ExportaciÃ³n**: Guardado en mÃºltiples formatos (Excel, CSV, JSON)

## ğŸ“Š CaracterÃ­sticas Principales

- âœ… **Trazabilidad Completa**: Todos los datos mantienen conexiÃ³n con `doc_id` y `accession`
- âœ… **Procesamiento BilingÃ¼e**: Soporta inglÃ©s y espaÃ±ol
- âœ… **MÃºltiples Visualizaciones**: GrÃ¡ficos estÃ¡ticos e interactivos
- âœ… **ExportaciÃ³n MÃºltiple**: Excel, CSV y JSON para diferentes usos
- âœ… **Listo para Web**: Formato JSON estructurado para aplicaciones web

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### LibrerÃ­as de Python

- **Procesamiento de Datos**: `pandas`, `numpy`
- **NLP**: `nltk`, `spacy`
- **VisualizaciÃ³n**: `matplotlib`, `seaborn`, `plotly`, `wordcloud`
- **Manejo de Archivos**: `openpyxl`, `xlsxwriter`

### Modelos

- **spaCy**: `en_core_web_sm` (modelo de inglÃ©s)
- **NLTK**: Stopwords en inglÃ©s y espaÃ±ol

## ğŸš€ InstalaciÃ³n

### OpciÃ³n 1: InstalaciÃ³n AutomÃ¡tica (Recomendada)

```powershell
# Ejecutar el script de instalaciÃ³n
.\setup.ps1
```

### OpciÃ³n 2: InstalaciÃ³n Manual

```powershell
# Instalar dependencias
pip install -r requirements.txt

# Descargar modelo de spaCy
python -m spacy download en_core_web_sm

# Descargar recursos NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

## ğŸ“– Uso

1. **Preparar los datos**: Coloca tu archivo JSON en el directorio del proyecto

2. **Ejecutar el notebook**: Abre `procesamiento.ipynb` en Jupyter/VS Code

3. **Ejecutar celda por celda**: Sigue el flujo del notebook

4. **Revisar resultados**: Los archivos se generarÃ¡n automÃ¡ticamente

## ğŸ“ Estructura de Archivos de Salida

```
documentos_metadata.xlsx/.csv       # Metadatos de documentos procesados
lemmas_expandido.xlsx/.csv          # DataFrame expandido (un lemma por fila)
frecuencias_lemmas.xlsx/.csv        # Conteo de frecuencias de palabras
bigramas_frecuentes.xlsx/.csv       # Pares de palabras mÃ¡s comunes
documentos_procesados.json          # Datos completos en formato JSON
```

## ğŸ” Formato de Datos

### DataFrame Principal (documentos_metadata)

| Campo | DescripciÃ³n |
|-------|-------------|
| `doc_id` | ID Ãºnico del documento |
| `accession` | CÃ³digo de acceso del estudio |
| `study_identifier` | Identificador del estudio |
| `study_title` | TÃ­tulo del estudio |
| `study_description` | DescripciÃ³n completa |
| `managing_center` | Centro NASA responsable |
| `project_type` | Tipo de proyecto |
| `num_tokens` | Cantidad de tokens |
| `num_lemmas` | Cantidad de lemmas |

### DataFrame Expandido (lemmas_expandido)

| Campo | DescripciÃ³n |
|-------|-------------|
| `doc_id` | ID del documento origen |
| `accession` | CÃ³digo de acceso |
| `lemma` | Palabra lematizada |

## ğŸ“ˆ Visualizaciones Incluidas

1. **GrÃ¡fico de Barras Horizontal**: Top 30 palabras mÃ¡s frecuentes
2. **GrÃ¡fico Interactivo (Plotly)**: ExploraciÃ³n dinÃ¡mica de frecuencias
3. **WordCloud ClÃ¡sica**: Nube de palabras estilo cientÃ­fico
4. **WordCloud Espacial**: Tema oscuro tipo espacio
5. **GrÃ¡fico de Bigramas**: Pares de palabras mÃ¡s comunes

## ğŸŒ Demo de BÃºsqueda (FastAPI + Streamlit)

Se incluye una demo ligera en `streamlit_app.py` que consume el endpoint POST `/studies/search` de la API FastAPI.

### Ejecutar API
```powershell
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Ejecutar Streamlit
```powershell
streamlit run streamlit_app.py
```

Luego abre el navegador (la terminal mostrarÃ¡ la URL local, tÃ­picamente http://localhost:8501) y ajusta filtros en el sidebar. Puedes cambiar la base de la API (por ejemplo un tÃºnel ngrok) en el campo "API Base URL".

CaracterÃ­sticas:
- BÃºsqueda reactiva por `q` y filtros (`organism`, `project_type`, `keywords`).
- Vista de artÃ­culos importantes, pÃ¡gina de resultados y suggested keywords.
- Modo compacto para reducir payload (`compact=true`).

Para exponer la demo fuera de tu red local puedes usar ngrok:
```powershell
ngrok http 8501
```

Si quieres proteger endpoints sensibles, aÃ±ade una verificaciÃ³n de API key en FastAPI antes de exponer pÃºblicamente.

## ğŸ’¡ PrÃ³ximos Pasos

DespuÃ©s de completar este procesamiento base, puedes:

1. **Aplicar filtros especÃ­ficos** segÃºn los tÃ³picos identificados
2. **Realizar anÃ¡lisis de tÃ³picos** (LDA, NMF)
3. **Clustering de documentos** similares
4. **AnÃ¡lisis temporal** si hay fechas disponibles
5. **Integrar con otros datasets** JSON

## ğŸ”§ PersonalizaciÃ³n

### Agregar Stopwords Personalizadas

```python
custom_stopwords = {'palabra1', 'palabra2', 'palabra3'}
stopwords_combined = stopwords_combined.union(custom_stopwords)
```

### Cambiar Longitud MÃ­nima de Tokens

```python
# En la funciÃ³n tokenize_and_filter
filtered_tokens = [token for token in tokens 
                   if len(token) >= 4]  # Cambiar de 3 a 4
```

### Ajustar WordCloud

```python
wordcloud = WordCloud(
    width=2000,           # Cambiar ancho
    height=1000,          # Cambiar alto
    max_words=200,        # MÃ¡s palabras
    colormap='inferno'    # Cambiar paleta
)
```

## ğŸ“ Notas Importantes

- âš ï¸ El procesamiento puede tardar varios minutos dependiendo del tamaÃ±o del dataset
- âš ï¸ Se requiere conexiÃ³n a internet para la instalaciÃ³n inicial
- âš ï¸ Los archivos JSON deben estar en formato UTF-8
- âš ï¸ La lematizaciÃ³n con spaCy puede consumir bastante memoria con datasets grandes

## ğŸ› SoluciÃ³n de Problemas

### Error: "Model 'en_core_web_sm' not found"

```powershell
python -m spacy download en_core_web_sm
```

### Error: "Resource 'stopwords' not found"

```powershell
python -c "import nltk; nltk.download('stopwords')"
```

### Error de Memoria

Si tienes problemas de memoria con datasets grandes:
- Procesa en lotes mÃ¡s pequeÃ±os
- Desactiva la lematizaciÃ³n (usa tokens directamente)
- Aumenta la memoria virtual de Python

## ğŸ‘¨â€ğŸ’» Autor

Proyecto desarrollado para la competiciÃ³n NASA 2025 - AnÃ¡lisis de BiologÃ­a Espacial

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto para uso educativo y de investigaciÃ³n.

---

**Â¡Buena suerte con la competiciÃ³n NASA! ğŸš€ğŸŒŸ**
